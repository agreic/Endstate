import json
import asyncio
from typing import Any

class MockLLMResponse:
    def __init__(self, content: str):
        self.content = content

class MockLLM:
    """
    A mock LLM that returns canned responses based on prompt keywords.
    Implements a subset of the LangChain Runnable interface (ainvoke).
    """
    def __init__(self):
        self.call_history = []

    async def ainvoke(self, input: Any, config: dict | None = None, **kwargs) -> MockLLMResponse:
        # Input can be a string, list of messages, or list of tuples
        # Standardize to the last message content/prompt
        prompt = ""
        if isinstance(input, str):
            prompt = input
        elif isinstance(input, list):
            last = input[-1]
            if isinstance(last, tuple):
                prompt = last[1]
            elif hasattr(last, "content"):
                prompt = last.content
            elif isinstance(last, dict):
                 prompt = last.get("content", "")
            else:
                prompt = str(last)
        else:
            prompt = str(input)

        self.call_history.append(prompt)
        prompt_lower = prompt.lower()

        # 1. Project Suggestions
        if "curriculum architect" in prompt_lower or ("json schema" in prompt_lower and "projects" in prompt_lower):
            return MockLLMResponse(json.dumps({
                "projects": [
                    {
                        "title": "Mock Project Alpha",
                        "description": "A mock project for testing integration flow.",
                        "difficulty": "Beginner",
                        "tags": ["Python", "Mocking"]
                    },
                    {
                        "title": "Mock Project Beta",
                        "description": "Another mock project for rigorous testing.",
                        "difficulty": "Intermediate",
                        "tags": ["Testing", "Automation"]
                    },
                    {
                        "title": "Mock Project Gamma",
                        "description": "A third advanced mock project.",
                        "difficulty": "Advanced",
                        "tags": ["AI", "MockLLM"]
                    }
                ]
            }))
        
        # 2. Lesson Generation (Check explicit intent)
        if "create" in prompt_lower and "lesson" in prompt_lower and "explanation" in prompt_lower:
             return MockLLMResponse(json.dumps({
                "explanation": "This is a mock lesson explanation generated by the MockLLM provider. It covers the core concept in a bite-sized format.",
                "task": "Read the provided documentation and reflect on the mock integration."
            }))

        # 3. Assessment Generation
        if "create one assessment prompt" in prompt_lower:
            return MockLLMResponse(json.dumps({
                "prompt": "What is the primary purpose of the MockLLM provider?"
            }))
            
        # 4. Assessment Evaluation
        if "evaluate the learner's answer" in prompt_lower:
            return MockLLMResponse(json.dumps({
                "result": "pass",
                "feedback": "Correct answer! The MockLLM provider enables deterministic testing without external dependencies."
            }))

        # 5. Capstone Evaluation
        if "capstone" in prompt_lower or ("rubric" in prompt_lower and "submission" in prompt_lower):
             return MockLLMResponse(json.dumps({
                "score": 0.95,
                "criteria": {"Skill Application": "Excellent"},
                "skill_evidence": {"Python": "Demonstrated"},
                "overall_feedback": "Great job on the mock project! You demonstrated usage of internal tools.",
                "suggestions": ["Add more integration tests"],
                "passed": True
            }))

        # 6. Project Alignment 
        if "alignment" in prompt_lower or "user goal" in prompt_lower:
             return MockLLMResponse(json.dumps({
                "score": 0.8,
                "critique": "Aligned well with the mock goal."
            }))

        # 7. KG Quality
        if "kg quality" in prompt_lower:
             return MockLLMResponse(json.dumps({
                "coverage_score": 0.9,
                "redundancy_score": 0.1,
                "critique": "Solid graph structure."
            }))
            
        # Default Chat
        return MockLLMResponse("This is a mock response from the AI. I am running in local test mode.")
