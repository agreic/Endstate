"""
LLM provider module for Endstate.
Supports Ollama (local) and Gemini (API) providers.
"""
from typing import Optional, Union
from enum import Enum

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage
import json

from ..config import LLMConfig, config


class LLMProvider(str, Enum):
    """Supported LLM providers."""
    OLLAMA = "ollama"
    GEMINI = "gemini"
    MOCK = "mock"


def get_llm(
    provider: Optional[Union[str, LLMProvider]] = None,
    llm_config: Optional[LLMConfig] = None,
    **kwargs,
) -> BaseChatModel:
    """
    Get an LLM instance based on provider.
    
    Args:
        provider: LLM provider ("ollama" or "gemini"). Uses config default if not specified.
        llm_config: Optional config override. Uses global config if not provided.
        **kwargs: Additional arguments passed to the LLM constructor.
        
    Returns:
        LangChain chat model instance.
        
    Raises:
        ValueError: If provider is not supported or API key is missing.
    """
    llm_config = llm_config or config.llm
    provider = provider or llm_config.provider
    
    if isinstance(provider, str):
        provider = LLMProvider(provider.lower())
    
    if provider == LLMProvider.OLLAMA:
        return _get_ollama_llm(llm_config, **kwargs)
    elif provider == LLMProvider.GEMINI:
        return _get_gemini_llm(llm_config, **kwargs)
    elif provider == LLMProvider.MOCK:
        return MockChatModel()
    else:
        raise ValueError(f"Unsupported LLM provider: {provider}")


def _get_ollama_llm(llm_config: LLMConfig, **kwargs) -> BaseChatModel:
    """Get Ollama LLM instance."""
    from langchain_ollama import ChatOllama
    
    ollama_config = llm_config.ollama
    timeout = kwargs.get("timeout_seconds", llm_config.timeout_seconds)
    keep_alive = kwargs.get("keep_alive", ollama_config.keep_alive)
    
    return ChatOllama(
        model=kwargs.get("model", ollama_config.model),
        base_url=kwargs.get("base_url", ollama_config.base_url),
        temperature=kwargs.get("temperature", ollama_config.temperature),
        keep_alive=keep_alive,
        sync_client_kwargs={"timeout": timeout},
        async_client_kwargs={"timeout": timeout},
        **{k: v for k, v in kwargs.items() if k not in ["model", "base_url", "temperature", "timeout", "timeout_seconds", "keep_alive"]},
    )


def _get_gemini_llm(llm_config: LLMConfig, **kwargs) -> BaseChatModel:
    """Get Gemini LLM instance."""
    from langchain_google_genai import ChatGoogleGenerativeAI
    
    gemini_config = llm_config.gemini
    
    # Check for API key
    api_key = kwargs.get("api_key") or gemini_config.api_key
    if not api_key:
        raise ValueError(
            "Gemini API key not found. Set GOOGLE_API_KEY environment variable, "
            "provide it in the request headers, or pass api_key parameter."
        )
    
    return ChatGoogleGenerativeAI(
        model=kwargs.get("model", gemini_config.model),
        temperature=kwargs.get("temperature", gemini_config.temperature),
        google_api_key=api_key,
        **{k: v for k, v in kwargs.items() if k not in ["model", "temperature", "api_key", "timeout", "google_api_key"]},
    )


def test_llm(llm: BaseChatModel) -> tuple[bool, str]:
    """
    Test an LLM connection.
    
    Args:
        llm: LLM instance to test.
        
    Returns:
        Tuple of (success: bool, message: str)
    """
    try:
        response = llm.invoke("Say hello in one word")
        return True, response.content
    except Exception as e:
        return False, str(e)


class MockChatModel:
    """Lightweight mock LLM for offline testing."""

    @property
    def _llm_type(self) -> str:
        return "mock"

    def _normalize_input(self, messages: object) -> str:
        if isinstance(messages, str):
            return messages
        if isinstance(messages, list):
            chunks = []
            for msg in messages:
                if isinstance(msg, tuple) and len(msg) == 2:
                    chunks.append(str(msg[1]))
                elif hasattr(msg, "content"):
                    chunks.append(str(msg.content))
                else:
                    chunks.append(str(msg))
            return "\n".join(chunks)
        return str(messages)

    def _response_for_prompt(self, prompt: str) -> str:
        lower = prompt.lower()
        if "chief curriculum architect" in lower or "\"projects\"" in lower:
            return json.dumps({
                "projects": [
                    {
                        "title": "Mock Starter Project",
                        "description": "A simple, focused project generated by the mock LLM.",
                        "difficulty": "Beginner",
                        "tags": ["Python", "CLI"],
                    },
                    {
                        "title": "Mock Stretch Project",
                        "description": "A slightly more ambitious mock project.",
                        "difficulty": "Intermediate",
                        "tags": ["Python", "SQLite"],
                    },
                    {
                        "title": "Mock Creative Project",
                        "description": "An imaginative mock project for experimentation.",
                        "difficulty": "Advanced",
                        "tags": ["Python", "Flask"],
                    },
                ]
            })
        if "create a short, focused lesson" in lower:
            return json.dumps({
                "title": "Mock Lesson",
                "explanation": "This is a mock lesson explanation focused on the selected node.",
            })
        if "create one assessment prompt" in lower:
            return json.dumps({
                "prompt": "Mock assessment prompt: explain the key concept in your own words."
            })
        if "evaluate the learner's answer" in lower or "evaluate the learner" in lower:
            return json.dumps({
                "result": "pass",
                "feedback": "Mock evaluation: sufficient understanding demonstrated."
            })
        if "capstone submission" in lower or "evaluate using this rubric" in lower:
            return json.dumps({
                "score": 0.75,
                "criteria": {
                    "skill_application": {"score": 0.7, "strengths": ["Clear intent"], "weaknesses": [], "suggestions": []},
                    "conceptual_understanding": {"score": 0.8, "strengths": ["Explains concepts"], "weaknesses": [], "suggestions": []},
                    "completeness": {"score": 0.75, "strengths": ["Covers scope"], "weaknesses": [], "suggestions": []},
                },
                "skill_evidence": {"Python": "Mentioned usage in core logic."},
                "overall_feedback": "Mock feedback: good progress, add more concrete details.",
                "suggestions": ["Add code snippets or specific implementation details."],
            })
        # Remediation diagnosis
        if "learning diagnostician" in lower or "analyze why a learner failed" in lower:
            return json.dumps({
                "diagnosis": "Confusion between base case and recursive case in recursion concept",
                "missing_concepts": ["Understanding Base Cases", "Recursion Fundamentals"],
                "severity": "moderate",
                "recommended_action": "insert_prerequisite"
            })
        # Remediation content generation  
        if "remediation lesson" in lower or "learner who struggled" in lower:
            return json.dumps({
                "name": "Understanding Base Cases",
                "description": "A focused review of base case concepts essential for recursion.",
                "explanation": "Base cases are the foundation of recursive thinking. A base case is the condition where the recursion stops. Without proper base cases, recursion leads to infinite loops."
            })
        return "Mock response: thanks for the message. Please continue."

    def invoke(self, messages: object, **_: object) -> AIMessage:
        prompt = self._normalize_input(messages)
        return AIMessage(content=self._response_for_prompt(prompt))

    async def ainvoke(self, messages: object, **_: object) -> AIMessage:
        prompt = self._normalize_input(messages)
        return AIMessage(content=self._response_for_prompt(prompt))
